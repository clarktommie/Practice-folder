{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dde3cef",
   "metadata": {},
   "source": [
    "## Model Validation and Cross-Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd4324",
   "metadata": {},
   "source": [
    "In this lab, we explore some techniques for model evaluation. Some of the commands in this lab may take a while to run on your computer.\n",
    "This file is drawn from labs that are part of the book that goes with the ISLP package.\n",
    "\n",
    "[<https://github.com/intro-stat-learning/ISLP_labs/blob/stable/Ch05-resample-lab.ipynb>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1deb5cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:13.493284Z",
     "iopub.status.busy": "2024-06-04T23:19:13.492950Z",
     "iopub.status.idle": "2024-06-04T23:19:14.143174Z",
     "shell.execute_reply": "2024-06-04T23:19:14.142882Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa08b62",
   "metadata": {},
   "source": [
    "There are several new imports needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "268c41b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.144884Z",
     "iopub.status.busy": "2024-06-04T23:19:14.144773Z",
     "iopub.status.idle": "2024-06-04T23:19:14.146541Z",
     "shell.execute_reply": "2024-06-04T23:19:14.146330Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "     (cross_validate,\n",
    "      KFold,\n",
    "      ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04f8e4",
   "metadata": {},
   "source": [
    "## The Test Set Approach\n",
    "We explore the use of the test or validation set approach in order to estimate\n",
    "the test error rates that result from fitting various linear models on\n",
    "the  `Auto`  data set.\n",
    "\n",
    "We use the function `train_test_split()` to split\n",
    "the data into training and validation sets. As there are 392 observations,\n",
    "we split into two equal sets of size 196 using the\n",
    "argument `test_size=196`. It is generally a good idea to set a random seed\n",
    "when performing operations like this that contain an\n",
    "element of randomness, so that the results obtained can be reproduced\n",
    "precisely at a later time. We set the random seed of the splitter\n",
    "with the argument `random_state=0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22f44ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.147809Z",
     "iopub.status.busy": "2024-06-04T23:19:14.147730Z",
     "iopub.status.idle": "2024-06-04T23:19:14.152606Z",
     "shell.execute_reply": "2024-06-04T23:19:14.152414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 392 entries, chevrolet chevelle malibu to chevy s-10\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           392 non-null    float64\n",
      " 1   cylinders     392 non-null    int64  \n",
      " 2   displacement  392 non-null    float64\n",
      " 3   horsepower    392 non-null    int64  \n",
      " 4   weight        392 non-null    int64  \n",
      " 5   acceleration  392 non-null    float64\n",
      " 6   year          392 non-null    int64  \n",
      " 7   origin        392 non-null    int64  \n",
      "dtypes: float64(3), int64(5)\n",
      "memory usage: 27.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Auto = load_data('Auto')\n",
    "print(Auto.info())\n",
    "Auto_train, Auto_test = train_test_split(Auto,\n",
    "                                         test_size=196,\n",
    "                                         random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fe69f",
   "metadata": {},
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set `Auto_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c32e917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.153847Z",
     "iopub.status.busy": "2024-06-04T23:19:14.153757Z",
     "iopub.status.idle": "2024-06-04T23:19:14.157537Z",
     "shell.execute_reply": "2024-06-04T23:19:14.157339Z"
    }
   },
   "outputs": [],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e883b8f",
   "metadata": {},
   "source": [
    "We now use the `predict()` method of `results` evaluated on the model matrix for this model\n",
    "created using the test data set. We also calculate the test MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86ce4f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.158717Z",
     "iopub.status.busy": "2024-06-04T23:19:14.158637Z",
     "iopub.status.idle": "2024-06-04T23:19:14.162177Z",
     "shell.execute_reply": "2024-06-04T23:19:14.161910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.61661706966988"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = hp_mm.transform(Auto_test)\n",
    "y_valid = Auto_test['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecdee6",
   "metadata": {},
   "source": [
    "Hence our estimate for the test MSE of  the linear regression\n",
    "fit is $23.62$.\n",
    "\n",
    "We can also estimate the test error for\n",
    "higher-degree polynomial regressions. We first provide a function `evalMSE()` that takes a model string as well\n",
    "as a training and test set and returns the MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50a66a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.163466Z",
     "iopub.status.busy": "2024-06-04T23:19:14.163397Z",
     "iopub.status.idle": "2024-06-04T23:19:14.165323Z",
     "shell.execute_reply": "2024-06-04T23:19:14.165076Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function call evalMSE\n",
    "def evalMSE(terms,\n",
    "            response,\n",
    "            train,\n",
    "            test):\n",
    "   # create the matrix needed, mm, based upon the terms in the model\n",
    "   mm = MS(terms)\n",
    "   # make training data\n",
    "   X_train = mm.fit_transform(train)\n",
    "   y_train = train[response]\n",
    "\n",
    "   # make test data\n",
    "   X_test = mm.transform(test)\n",
    "   y_test = test[response]\n",
    "\n",
    "   # fit the regression model \n",
    "   results = sm.OLS(y_train, X_train).fit()\n",
    "   # get the predicted values from the model fit above on the test data\n",
    "   test_pred = results.predict(X_test)\n",
    "   # return the RMSE\n",
    "   return np.mean((y_test - test_pred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255779c",
   "metadata": {},
   "source": [
    "Let’s use this function to estimate the test MSE\n",
    "using linear, quadratic, cubic and quartic fits. We use the `enumerate()`  function\n",
    "here, which gives both the values and indices of objects as one iterates\n",
    "over a _for loop_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d49b6999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.166563Z",
     "iopub.status.busy": "2024-06-04T23:19:14.166497Z",
     "iopub.status.idle": "2024-06-04T23:19:14.177198Z",
     "shell.execute_reply": "2024-06-04T23:19:14.176975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163, 18.77852784])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a blank array of all zeroes of length 3\n",
    "MSE = np.zeros(4)\n",
    "# create a for loop over the values \n",
    "for idx, degree in enumerate(range(1, 5)):\n",
    "    # fit different models to \n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "                       'mpg',\n",
    "                       Auto_train,\n",
    "                       Auto_test)\n",
    "MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b8fc1",
   "metadata": {},
   "source": [
    "These error rates are $23.62, 18.76$, $18.80$, and $18.78$ respectively. If we\n",
    "choose a different training/validation split instead, then we\n",
    "can expect somewhat different errors on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dac8bd54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.178405Z",
     "iopub.status.busy": "2024-06-04T23:19:14.178321Z",
     "iopub.status.idle": "2024-06-04T23:19:14.188650Z",
     "shell.execute_reply": "2024-06-04T23:19:14.188432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833, 16.89589193])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_train, Auto_test = train_test_split(Auto,\n",
    "                                          test_size=196,\n",
    "                                          random_state=3)\n",
    "MSE = np.zeros(4)\n",
    "for idx, degree in enumerate(range(1, 5)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "                       'mpg',\n",
    "                       Auto_train,\n",
    "                       Auto_test)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2c12d",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation set,\n",
    "we find that the validation set error rates for the models with linear, quadratic, and cubic terms are $20.76$, $16.95$,$16.97$, and $16.90$ respectively.\n",
    "\n",
    "Seems like there is not much advantage to using the cubic or quartic models over the\n",
    "quadratic model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22daa51",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "In theory, the cross-validation estimate can be computed for any generalized\n",
    "linear model.  {}\n",
    "In practice, however, the simplest way to cross-validate in\n",
    "Python is to use `sklearn`, which has a different interface or API\n",
    "than `statsmodels`, the code we have been using to fit models.\n",
    "\n",
    "This is a problem which often confronts data scientists: \"I have a function to do task $A$, and need to feed it into something that performs task $B$, so that I can compute $B(A(D))$, where $D$ is my data.\" When $A$ and $B$ don’t naturally speak to each other, this\n",
    "requires the use of a *wrapper*.\n",
    "In the `ISLP` package,\n",
    "we provide \n",
    "a wrapper, `sklearn_sm()`, that enables us to easily use the cross-validation tools of `sklearn` with\n",
    "models fit by `statsmodels`.\n",
    "\n",
    "The class `sklearn_sm()` \n",
    "has  as its first argument\n",
    "a model from `statsmodels`. It can take two additional\n",
    "optional arguments: `model_str` which can be\n",
    "used to specify a formula, and `model_args` which should\n",
    "be a dictionary of additional arguments used when fitting\n",
    "the model. For example, to fit a logistic regression model\n",
    "we have to specify a `family` argument. This\n",
    "is passed as `model_args={'family':sm.families.Binomial()}`.\n",
    "\n",
    "Here is our wrapper in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "601ae443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.189993Z",
     "iopub.status.busy": "2024-06-04T23:19:14.189906Z",
     "iopub.status.idle": "2024-06-04T23:19:14.876368Z",
     "shell.execute_reply": "2024-06-04T23:19:14.876129Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.23151351792922"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model = sklearn_sm(sm.OLS,\n",
    "                      MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "                            X,\n",
    "                            Y,\n",
    "                            cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebadc35f",
   "metadata": {},
   "source": [
    "The arguments to `cross_validate()` are as follows: an\n",
    "object with the appropriate `fit()`, `predict()`,\n",
    "and `score()` methods,  an\n",
    "array of features `X` and a response `Y`. \n",
    "We also included an additional argument `cv` to `cross_validate()`; specifying an integer\n",
    "$K$ results in $K$-fold cross-validation. We have provided a value \n",
    "corresponding to the total number of observations, which results in\n",
    "leave-one-out cross-validation (LOOCV). The `cross_validate()`  function produces a dictionary with several components;\n",
    "we simply want the cross-validated test score here (MSE), which is estimated to be 24.23."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f47b99",
   "metadata": {},
   "source": [
    "We can repeat this procedure for increasingly complex polynomial fits.\n",
    "To automate the process, we again\n",
    "use a for loop which iteratively fits polynomial\n",
    "regressions of degree 1 to 5, computes the\n",
    "associated cross-validation error, and stores it in the $i^{th}$ element\n",
    "of the vector `cv_error`. The variable `d` in the _for loop_\n",
    "corresponds to the degree of the polynomial. We begin by initializing the\n",
    "vector. This command may take a couple of seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11226c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:14.877800Z",
     "iopub.status.busy": "2024-06-04T23:19:14.877726Z",
     "iopub.status.idle": "2024-06-04T23:19:15.384419Z",
     "shell.execute_reply": "2024-06-04T23:19:15.384193Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.4244303 , 19.03322528])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a920ae",
   "metadata": {},
   "source": [
    "We see a sharp drop in the estimated test MSE between the linear and\n",
    "quadratic fits, but then no clear improvement from using higher-degree polynomials.\n",
    "\n",
    "Above we introduced the `outer()`  method of the `np.power()`\n",
    "function.  The `outer()` method is applied to an operation\n",
    "that has two arguments, such as `add()`, `min()`, or\n",
    "`power()`.\n",
    "It has two arrays as arguments, and then forms a larger\n",
    "array where the operation is applied to each pair of elements of the\n",
    "two arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64b64d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:15.385768Z",
     "iopub.status.busy": "2024-06-04T23:19:15.385690Z",
     "iopub.status.idle": "2024-06-04T23:19:15.387686Z",
     "shell.execute_reply": "2024-06-04T23:19:15.387484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  7],\n",
       "       [ 7,  9],\n",
       "       [11, 13]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([3, 5, 9])\n",
    "B = np.array([2, 4])\n",
    "np.add.outer(A, B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71385c1b",
   "metadata": {},
   "source": [
    "In the CV example above, we used $K=n$, but of course we can also use $K<n$. The code is very similar\n",
    "to the above (and is significantly faster). Here we use `KFold()` to partition the data into $K=10$ random groups. We use `random_state` to set a random seed and initialize a vector `cv_error` in which we will store the CV errors corresponding to the\n",
    "polynomial fits of degrees one to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca0f972f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:15.389014Z",
     "iopub.status.busy": "2024-06-04T23:19:15.388934Z",
     "iopub.status.idle": "2024-06-04T23:19:15.407438Z",
     "shell.execute_reply": "2024-06-04T23:19:15.407194Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13719048])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b234093",
   "metadata": {},
   "source": [
    "Notice that the computation time is much shorter than that of LOOCV.\n",
    "(In principle, the computation time for LOOCV for a least squares\n",
    "linear model should be faster than for $K$-fold CV)  \n",
    "We still see little evidence that using cubic\n",
    "or higher-degree polynomial terms leads to a lower test error than simply\n",
    "using a quadratic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4487a4",
   "metadata": {},
   "source": [
    "The `cross_validate()` function is flexible and can take\n",
    "different splitting mechanisms as an argument. For instance, one can use the `ShuffleSplit()` funtion to implement\n",
    "the test/validation set approach just as easily as K-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "080cdb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:15.408750Z",
     "iopub.status.busy": "2024-06-04T23:19:15.408677Z",
     "iopub.status.idle": "2024-06-04T23:19:15.413979Z",
     "shell.execute_reply": "2024-06-04T23:19:15.413762Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=1,\n",
    "                          test_size=196,\n",
    "                          random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop(['mpg'], axis=1),\n",
    "                         Auto['mpg'],\n",
    "                         cv=validation);\n",
    "results['test_score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4b4cf",
   "metadata": {},
   "source": [
    "One can estimate the variability in the test error by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c46de2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T23:19:15.415225Z",
     "iopub.status.busy": "2024-06-04T23:19:15.415158Z",
     "iopub.status.idle": "2024-06-04T23:19:15.437526Z",
     "shell.execute_reply": "2024-06-04T23:19:15.437302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.802232661034164, 1.4218450941091831)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10,\n",
    "                          test_size=196,\n",
    "                          random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop(['mpg'], axis=1),\n",
    "                         Auto['mpg'],\n",
    "                         cv=validation)\n",
    "results['test_score'].mean(), results['test_score'].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d505e1a",
   "metadata": {},
   "source": [
    "Note that this standard deviation is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the randomly-selected training samples overlap and hence introduce correlations. But it does give an idea of the Monte Carlo variation incurred by picking different random folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf5fb4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13719048])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30b7b688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.81114253, 22.2694531 , 23.94237084, 27.92859286, 22.62260018,\n",
       "       30.0976095 , 23.87376114, 27.77074161, 14.73007962, 27.03029353])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=0) # use same splits for each degree\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop(['mpg'], axis=1),\n",
    "                         Auto['mpg'],\n",
    "                         cv=cv);\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "93d539fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 22.96552529, 23.43853845, 21.72781699, 22.79416823,\n",
       "       23.09191932, 23.69196999, 23.90184611, 26.53545818, 26.258467  ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = ShuffleSplit(n_splits=10,\n",
    "                          test_size=196,\n",
    "                          random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop(['mpg'], axis=1),\n",
    "                         Auto['mpg'],\n",
    "                         cv=cv)\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef9ae9",
   "metadata": {},
   "source": [
    "Now we will do some cross validation on regression with the penguins data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0acc58e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 333 entries, 0 to 343\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            333 non-null    object \n",
      " 1   island             333 non-null    object \n",
      " 2   bill_length_mm     333 non-null    float64\n",
      " 3   bill_depth_mm      333 non-null    float64\n",
      " 4   flipper_length_mm  333 non-null    float64\n",
      " 5   body_mass_g        333 non-null    float64\n",
      " 6   sex                333 non-null    object \n",
      " 7   year               333 non-null    int64  \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 23.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "penguins = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/penguins.csv\", na_values=['NA'])\n",
    "# remove rows with missing data\n",
    "penguins.dropna(inplace=True)\n",
    "penguins.head()\n",
    "print(penguins.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5df7eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pens_model = sklearn_sm(sm.OLS)\n",
    "X = penguins.drop(['species','island',\n",
    "                   'body_mass_g','sex','year'],axis=1)\n",
    "\n",
    "Y = penguins['body_mass_g'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd10dfd",
   "metadata": {},
   "source": [
    "For comparison we will build a linear regression with all of the data and compare the performance from using all of the data to what we would get on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0d20e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 390.63708572458495\n"
     ]
    }
   ],
   "source": [
    "X = penguins[['bill_depth_mm', 'flipper_length_mm', 'bill_length_mm']]  \n",
    "y = penguins['body_mass_g']  \n",
    "\n",
    "\n",
    "# Create a linear regression model\n",
    "bluejay_model3 = LinearRegression()\n",
    "\n",
    "# Fit the model on the  data\n",
    "bluejay_model3.fit(X, y)\n",
    "\n",
    "# Make predictions on the  data\n",
    "y_hat = bluejay_model3.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = root_mean_squared_error(y, y_hat)\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0689460a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([468.03038214, 411.11002098, 426.66842704, 485.36520748,\n",
       "       472.61855398, 514.73290301])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross validate does not give RMSE as a possible output\n",
    "# but gives negative MSE, so we adjust that output\n",
    "cv = KFold(n_splits=6,\n",
    "           shuffle=True,\n",
    "           random_state=42)\n",
    "results = cross_validate(pens_model,\n",
    "                         X,\n",
    "                         Y,\n",
    "                         cv=cv,\n",
    "                         scoring=('neg_mean_squared_error'));\n",
    "np.sqrt(-1*results['test_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bccc6",
   "metadata": {},
   "source": [
    "The RMSE for the full data regression is $390.6$ while the average for the KFold is about $450$.  So we see a drop in performance but the idea is that the latter is more likely what we would observe in application of the model to new data.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7dad2",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Fit a multiple linear regression to *all* of the data that has flipper length and bill length as predictors.  Find the RMSE.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91631262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 391.62387324254576\n"
     ]
    }
   ],
   "source": [
    "model2=LinearRegression()\n",
    "X2 = penguins[['flipper_length_mm', 'bill_length_mm']]  \n",
    "y = penguins  \n",
    "X = sm.add_constant(X)\n",
    "\n",
    "\n",
    "# Fit the model on the  data\n",
    "model2.fit(X2, y)\n",
    "\n",
    "# Make predictions on the  data\n",
    "y_hat = model2.predict(X2)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = root_mean_squared_error(y, y_hat)\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabb644",
   "metadata": {},
   "source": [
    "\n",
    "2. Now do a K-fold cross validation of the model in Task 1 with 6 folds and find the RMSE.  How do your answers in this task compare to the answers in Task 1?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20e8076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562.2603459849262\n"
     ]
    }
   ],
   "source": [
    "X2 = penguins[['flipper_length_mm', 'bill_length_mm']] \n",
    "# Fit a multiple linear regression to *all* of the data that has flipper length and bill length as predictors.  Find the RMSE.  \n",
    "cv2 = KFold(n_splits=6,\n",
    "           shuffle=True,\n",
    "           random_state=42)\n",
    "results = cross_validate(pens_model,\n",
    "                         X2,\n",
    "                         Y,\n",
    "                         cv=cv2,\n",
    "                         scoring=('neg_mean_squared_error'));\n",
    "np.sqrt(-1*results['test_score'])\n",
    "# print the average RMSE\n",
    "print(np.mean(np.sqrt(-1*results['test_score'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a61b8a",
   "metadata": {},
   "source": [
    "\n",
    "2. Change the number of folds, *n_splits*, in task 1 from 6 to 10.  How does that change your RMSE results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52b1e7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561.478278910494\n"
     ]
    }
   ],
   "source": [
    "cv2 = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=42)\n",
    "results = cross_validate(pens_model,\n",
    "                         X2,\n",
    "                         Y,\n",
    "                         cv=cv2,\n",
    "                         scoring=('neg_mean_squared_error'));\n",
    "np.sqrt(-1*results['test_score'])\n",
    "# print the average RMSE\n",
    "print(np.mean(np.sqrt(-1*results['test_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ecc5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3. Change the random seed, *random_state*, in task 1 and 2 from 42 to 20250217.  How does that change your RMSE results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b589ea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564.3382327090437\n"
     ]
    }
   ],
   "source": [
    "cv2 = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=20250217)\n",
    "results = cross_validate(pens_model,\n",
    "                         X2,\n",
    "                         Y,\n",
    "                         cv=cv2,\n",
    "                         scoring=('neg_mean_squared_error'));\n",
    "np.sqrt(-1*results['test_score'])\n",
    "# print the average RMSE\n",
    "print(np.mean(np.sqrt(-1*results['test_score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e43ac2",
   "metadata": {},
   "source": [
    "\n",
    "4. Why might we want to change the number of splits in our code?  What are the advantages of a large number of folds and what are the advantages of a small number of folds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f187ee",
   "metadata": {},
   "source": [
    "The number of splits in cv affects how we analize our model. More folds like 10-fold give a more reliable estimate of performance because each data point gets used for both training and testing, but it can be slower and more computationally expensive. On the other hand, fewer folds (like 3-6) are faster but might give a less stable estimate, as the model gets trained on fewer data points. It’s all about balancing accuracy with speed!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
